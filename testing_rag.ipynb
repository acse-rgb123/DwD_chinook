{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Schema: {'titanic': {'PassengerId': 'INTEGER', 'Survived': 'INTEGER', 'Pclass': 'INTEGER', 'Name': 'TEXT', 'Sex': 'TEXT', 'Age': 'REAL', 'SibSp': 'INTEGER', 'Parch': 'INTEGER', 'Ticket': 'TEXT', 'Fare': 'REAL', 'Cabin': 'TEXT', 'Embarked': 'TEXT'}}\n",
            "Relevant columns: [('Fare: The fare paid for the ticket.', 0.35786408), ('Survived: Indicates whether the passenger survived (0 = No, 1 = Yes).', 0.38732126), ('Parch: Number of parents/children aboard the Titanic.', 0.3902487), ('Ticket: The ticket number of the passenger.', 0.4037013), ('Cabin: The cabin number where the passenger stayed.', 0.4108491), ('Age: Age of the passenger in years.', 0.418823), ('Pclass: Ticket class (1st, 2nd, 3rd).', 0.42880145), ('Name: The full name of the passenger.', 0.42984462), ('SibSp: Number of siblings/spouses aboard the Titanic.', 0.43792555), ('Sex: Gender of the passenger (male or female).', 0.44298968)]\n",
            "Relevant schema columns: [('Fare: The fare paid for the ticket.', 0.35786408), ('Survived: Indicates whether the passenger survived (0 = No, 1 = Yes).', 0.38732126), ('Parch: Number of parents/children aboard the Titanic.', 0.3902487), ('Ticket: The ticket number of the passenger.', 0.4037013), ('Cabin: The cabin number where the passenger stayed.', 0.4108491), ('Age: Age of the passenger in years.', 0.418823), ('Pclass: Ticket class (1st, 2nd, 3rd).', 0.42880145), ('Name: The full name of the passenger.', 0.42984462), ('SibSp: Number of siblings/spouses aboard the Titanic.', 0.43792555), ('Sex: Gender of the passenger (male or female).', 0.44298968)]\n",
            "Generated SQL Query: SELECT * FROM titanic WHERE survived = 1 AND fare > 50;\n",
            "SQL Query Results:\n",
            "     PassengerId  Survived  Pclass  \\\n",
            "0              2         1       1   \n",
            "1              4         1       1   \n",
            "2             32         1       1   \n",
            "3             53         1       1   \n",
            "4             62         1       1   \n",
            "..           ...       ...     ...   \n",
            "104          839         1       3   \n",
            "105          850         1       1   \n",
            "106          857         1       1   \n",
            "107          872         1       1   \n",
            "108          880         1       1   \n",
            "\n",
            "                                                  Name     Sex   Age  SibSp  \\\n",
            "0    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "1         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "2       Spencer, Mrs. William Augustus (Marie Eugenie)  female   NaN      1   \n",
            "3             Harper, Mrs. Henry Sleeper (Myna Haxtun)  female  49.0      1   \n",
            "4                                  Icard, Miss. Amelie  female  38.0      0   \n",
            "..                                                 ...     ...   ...    ...   \n",
            "104                                    Chip, Mr. Chang    male  32.0      0   \n",
            "105       Goldenberg, Mrs. Samuel L (Edwiga Grabowska)  female   NaN      1   \n",
            "106         Wick, Mrs. George Dennick (Mary Hitchcock)  female  45.0      1   \n",
            "107   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.0      1   \n",
            "108      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.0      0   \n",
            "\n",
            "     Parch    Ticket      Fare Cabin Embarked  \n",
            "0        0  PC 17599   71.2833   C85        C  \n",
            "1        0    113803   53.1000  C123        S  \n",
            "2        0  PC 17569  146.5208   B78        C  \n",
            "3        0  PC 17572   76.7292   D33        C  \n",
            "4        0    113572   80.0000   B28     None  \n",
            "..     ...       ...       ...   ...      ...  \n",
            "104      0      1601   56.4958  None        S  \n",
            "105      0     17453   89.1042   C92        C  \n",
            "106      1     36928  164.8667  None        S  \n",
            "107      1     11751   52.5542   D35        S  \n",
            "108      1     11767   83.1583   C50        C  \n",
            "\n",
            "[109 rows x 12 columns]\n",
            "Interpretation:\n",
            "The result of the SQL query provided is a list of passengers who survived and paid more than 50 dollars for their fare. Here's a brief numerical summary of the data:\n",
            "\n",
            "1. Total Passengers: The query result includes a total of 123 passengers who survived and paid more than 50 dollars for their fare.\n",
            "\n",
            "2. Gender Distribution: Out of these, 92 are female and 31 are male. Thus, approximately 75% of the passengers who survived and paid more than 50 dollars for their fare were female.\n",
            "\n",
            "3. Age Information: The age information is available for 97 out of 123 passengers. The youngest passenger is less than a year old and the oldest is 63 years old. \n",
            "\n",
            "4. Class Distribution: Majority of these passengers (about 96%) were in the 1st class (Pclass 1), indicating a strong correlation between fare, survival rate, and passenger class.\n",
            "\n",
            "5. Fare: The minimum fare paid by these passengers is 52 dollars and the maximum fare is approximately 512 dollars. \n",
            "\n",
            "6. Embarkation: Most of the passengers embarked from port 'C' (Cherbourg) and 'S' (Southampton).\n",
            "\n",
            "7. Cabin Information: Cabin information is available for about 77 passengers. Some passengers had multiple cabin numbers listed.\n",
            "\n",
            "8. Family: Most of the passengers were either travelling alone or with one other person (spouse or sibling). However, some passengers were travelling with up to 3 siblings/sp\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import faiss\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = 'sk-ko4mD0G5q5eQe7c_mzU2rd5jJnQbWxGCIThHBbZ_dnT3BlbkFJA8DjlC9WP2q3D2IMvaEKuQkWHQlPewnDjHbDr45NwA' \n",
        "\n",
        "# Step 1: Extract schema metadata (table names and column names) from the database\n",
        "def extract_schema_from_db(db_file):\n",
        "    conn = sqlite3.connect(db_file)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Get all table names from the database\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = cursor.fetchall()\n",
        "\n",
        "    schema = {}\n",
        "    for table in tables:\n",
        "        table_name = table[0]\n",
        "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "        columns = cursor.fetchall()\n",
        "\n",
        "        schema[table_name] = {}\n",
        "        for col in columns:\n",
        "            # col[1] is the column name, col[2] is the type (like int, text, OBJECT.)\n",
        "            schema[table_name][col[1]] = col[2]  # Store column name and its type\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"Extracted Schema: {schema}\")\n",
        "    return schema\n",
        "\n",
        "\n",
        "# Step 2: Vectorize schema metadata and store it in FAISS\n",
        "def create_vector_db_from_schema(schema):\n",
        "    schema_embeddings = []\n",
        "    column_descriptions = []\n",
        "\n",
        "    for table, columns in schema.items():\n",
        "        for column_name, column_type in columns.items():\n",
        "            # Combine table, column, and type for vectorization\n",
        "            description = f\"Table {table}, Column {column_name}, Type {column_type}\"\n",
        "            column_descriptions.append(description)\n",
        "            \n",
        "            # Get embedding for the description\n",
        "            embedding = openai.Embedding.create(input=description, model=\"text-embedding-ada-002\")['data'][0]['embedding']\n",
        "            schema_embeddings.append(embedding)\n",
        "\n",
        "    # Convert to numpy array for FAISS\n",
        "    schema_embeddings = np.array(schema_embeddings).astype('float32')\n",
        "\n",
        "    # Initialize FAISS index for schema embeddings\n",
        "    d = schema_embeddings.shape[1]  # Dimensionality of embeddings ROOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOM FOR CHAAAAAAAAAAAAAAAAAAANGE\n",
        "    index = faiss.IndexFlatIP(d)  # L2 distance index ROOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOM FOR CHAAAAAAAAAAAAAAAAAAAAAAAAANGE\n",
        "    index.add(schema_embeddings)\n",
        "\n",
        "    # Save the FAISS index and column descriptions\n",
        "    faiss.write_index(index, \"schema_index.faiss\")\n",
        "    np.save(\"column_descriptions.npy\", column_descriptions)\n",
        "\n",
        "    print(f\"Vectorized schema and saved to FAISS.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "# Step 3: Retrieve relevant columns and table names based on user query\n",
        "def retrieve_relevant_schema(query, schema_index_file=\"schema_index.faiss\", column_descriptions_file=\"column_descriptions.npy\", top_n=10):\n",
        "    # Load FAISS index and descriptions\n",
        "    index = faiss.read_index(schema_index_file)\n",
        "    column_descriptions = np.load(column_descriptions_file, allow_pickle=True)\n",
        "\n",
        "    # Get embedding for the user query\n",
        "    query_embedding = openai.Embedding.create(input=query, model=\"text-embedding-ada-002\")['data'][0]['embedding']\n",
        "    query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n",
        "\n",
        "    # Search for most relevant schema columns\n",
        "    D, I = index.search(query_embedding, len(column_descriptions))\n",
        "\n",
        "    # Return top N most relevant columns\n",
        "    relevant_columns = [(column_descriptions[i], D[0][idx]) for idx, i in enumerate(I[0][:top_n])]\n",
        "\n",
        "    if not relevant_columns:\n",
        "        print(\"No relevant columns found!\")\n",
        "    else:\n",
        "        print(f\"Relevant columns: {relevant_columns}\")\n",
        "\n",
        "    return relevant_columns\n",
        "\n",
        "\n",
        "# Step 4: Use GPT-4 to generate SQL based on relevant schema columns and table\n",
        "def generate_sql_prompt(query, relevant_columns):\n",
        "    table_name = None\n",
        "    for col, _ in relevant_columns:\n",
        "        if \"Table\" in col:\n",
        "            table_name = col.split(\",\")[0].replace(\"Table\", \"\").strip()\n",
        "            break\n",
        "\n",
        "    # Extract relevant columns\n",
        "    relevant_col_names = [col.split(\",\")[1].replace(\"Column\", \"\").strip() for col, _ in relevant_columns if \"Column\" in col]\n",
        "\n",
        "    # If no table name is found, assume a default\n",
        "    table_name = table_name or \"titanic\"\n",
        "\n",
        "    # Create the SQL generation prompt for GPT-4\n",
        "    context = f\"Relevant columns: {', '.join(relevant_col_names)}.\"\n",
        "    input_text = f\"Generate only a valid SQL query for the following question: {query}. Use the table '{table_name}' and the relevant columns. Do not include any explanation, only the SQL query.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in SQL generation.\"},\n",
        "            {\"role\": \"user\", \"content\": input_text}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "\n",
        "# Step 5: Extract SQL query from GPT-4 response\n",
        "def extract_sql_from_response(response_text):\n",
        "    try:\n",
        "        start_idx = response_text.upper().index(\"SELECT\")\n",
        "        end_idx = response_text.index(\";\", start_idx) + 1  # Find the first semicolon after SELECT\n",
        "        sql_query = response_text[start_idx:end_idx].strip()\n",
        "        return sql_query\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting SQL from response: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Step 6: Execute the SQL query against the SQLite database\n",
        "def execute_sql_query(db_file, query):\n",
        "    if not os.path.exists(db_file):\n",
        "        print(f\"Database file not found: {db_file}\")\n",
        "        return None\n",
        "\n",
        "    conn = sqlite3.connect(db_file)\n",
        "    try:\n",
        "        df = pd.read_sql_query(query, conn)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing query: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "\n",
        "# Step 7: Interpret the SQL result using GPT-4 to answer the original user question\n",
        "def interpret_results(results, user_query):\n",
        "    if results is None or results.empty:\n",
        "        return \"No results to interpret.\"\n",
        "\n",
        "    # Convert the results DataFrame to a string format for passing to GPT-4\n",
        "    results_str = results.to_string(index=False)\n",
        "\n",
        "    # Construct the input text to provide context for GPT-4 to analyze and answer the user query\n",
        "    input_text = f\"Given the following SQL query result:\\n{results_str}\\nBased on this data, answer the original user question: '{user_query}' as accurately and completely as possible. Also give a analytical  summary of the result at the end that is short but is more numerical in nature\"\n",
        "\n",
        "    # Call GPT-4 for interpreting the results and answering the original query\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in interpreting data and providing answers based on SQL results.\"},\n",
        "            {\"role\": \"user\", \"content\": input_text}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "\n",
        "    # Return the final answer based on the SQL data and the user's query\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "\n",
        "\n",
        "# Step 8: Full pipeline to run the entire process\n",
        "def run_pipeline(user_query, db_file):\n",
        "    # Step 1: Extract schema from the database\n",
        "    schema = extract_schema_from_db(db_file)\n",
        "\n",
        "    # Step 2: Vectorize the schema and store in FAISS (only once)\n",
        "    if not os.path.exists(\"schema_index.faiss\"):\n",
        "        create_vector_db_from_schema(schema)\n",
        "\n",
        "    # Step 3: Retrieve relevant schema columns based on user query\n",
        "    relevant_columns = retrieve_relevant_schema(user_query)\n",
        "    print(f\"Relevant schema columns: {relevant_columns}\")\n",
        "\n",
        "    # Step 4: Generate SQL query using GPT-4 based on the relevant schema\n",
        "    sql_query = generate_sql_prompt(user_query, relevant_columns)\n",
        "    print(f\"Generated SQL Query: {sql_query}\")\n",
        "\n",
        "    # Step 5: Execute SQL query\n",
        "    results = execute_sql_query(db_file, sql_query)\n",
        "    print(f\"SQL Query Results:\\n{results}\")\n",
        "\n",
        "    # Step 6: Interpret results\n",
        "    interpretation = interpret_results(results, user_query)\n",
        "    print(f\"Interpretation:\\n{interpretation}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "user_query = \"Find all passengers who survived and paid more than 50 dollars for their fare.\"\n",
        "db_file = 'titanic.db'  # Path to your SQLite database\n",
        "\n",
        "# Run the full pipeline\n",
        "run_pipeline(user_query, db_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "name": "Making the Most of your Colab Subscription",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
